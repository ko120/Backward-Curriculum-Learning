{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxsdYOtlchzc"
      },
      "source": [
        "# Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ykkKPog6QG6"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obx2tUVVco5a"
      },
      "source": [
        "# Actor Critic Share Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGfFj5m6HUsW"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class ActorCritic_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(ActorCritic_Net , self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims,fc1_dims)\n",
        "    self.actor = nn.Linear(fc1_dims, output_dims)\n",
        "    self.critic = nn.Linear(fc1_dims,1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    pi = F.softmax(self.actor(x), dim = 1)\n",
        "    value = self.critic(x)\n",
        "    return (pi, value)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhMTk1YtNhH"
      },
      "source": [
        "# Actor Critic Seperate Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrdQzMdgtQSC"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "class Actor_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(Actor_Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "    self.out = nn.Linear(fc1_dims, output_dims)\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.out.weight)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.softmax(self.out(x), dim = 1)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Critic_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(Critic_Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "    self.out = nn.Linear(fc1_dims, 1)\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.out.weight)\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = self.out(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WESaGvzgcroP"
      },
      "source": [
        "# REINFORCE with Baseline Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct-q7avTJhWi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical \n",
        "import numpy as np\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128, gamma = 0.99, ac_lr = 1e-3, cr_lr = 1e-2, optimizer = 'RMSprop'):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    self.actor_net = Actor_Net(input_dims= input_dims, output_dims= output_dims, fc1_dims= fc1_dims)\n",
        "    self.critic_net = Critic_Net(input_dims= input_dims, output_dims = output_dims, fc1_dims= fc1_dims)\n",
        "    if optimizer == 'RMSprop':\n",
        "      self.actor_optim = optim.RMSprop(params = self.actor_net.parameters(),lr = ac_lr)\n",
        "      self.critic_optim = optim.RMSprop(params= self.critic_net.parameters(), lr = cr_lr)\n",
        "    else:\n",
        "      self.actor_optim = optim.Adam(params = self.actor_net.parameters(),lr = ac_lr)\n",
        "      self.critic_optim = optim.Adam(params= self.critic_net.parameters(), lr = cr_lr)\n",
        "\n",
        "    self.gamma = gamma\n",
        "\n",
        "    self.log_probs = []\n",
        "    self.values = []\n",
        "  \n",
        "  def get_action(self, state):\n",
        "    with torch.no_grad():\n",
        "      pi = self.actor_net(state)\n",
        "      distribution = Categorical(probs = pi)\n",
        "      action = distribution.sample()\n",
        "      self.log_probs.append(distribution.log_prob(action))\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "  def learn_mean(self, rewards ,states, actions, return_norm = True):\n",
        "    \n",
        "    returns = []\n",
        "    states = torch.cat(states, dim = 0).to(device)\n",
        "    actions = torch.tensor(actions).to(device)  \n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards[::-1]:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "    # Resize the vectors\n",
        "  \n",
        "    #self.values = torch.cat(self.values).squeeze() # concatinating plus squeeze since input dim is 2d\n",
        "    self.log_probs = torch.cat(self.log_probs) # only concatinating since input dim is 1d\n",
        "\n",
        "    # Compute actor and critic losses\n",
        "    \n",
        "    for G, log_prob, state in zip(returns, self.log_probs, states):\n",
        "      G = G.detach() \n",
        "      v = self.critic_net(state)\n",
        "      advantage = G - v.item() # detach the grad computation to avoid computing gradient\n",
        "      actor_losses.append(-log_prob * advantage.detach())\n",
        "      critic_losses.append(F.smooth_l1_loss(v, torch.tensor([G]).to(device)))\n",
        "\n",
        "    self.critic_optim.zero_grad()\n",
        "    self.actor_optim.zero_grad()\n",
        "    critic_losses = torch.stack(critic_losses).to(device).mean()\n",
        "    critic_losses.backward()\n",
        "    actor_losses = torch.stack(actor_losses).to(device).mean()\n",
        "    actor_losses.backward()\n",
        "    self.critic_optim.step()\n",
        "    self.actor_optim.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n",
        "\n",
        "  def learn_forward(self, rewards, states, actions, return_norm = True):\n",
        "    \n",
        "  \n",
        "    returns = []\n",
        "    states = torch.cat(states, dim = 0).to(device)\n",
        "    actions = torch.tensor(actions).to(device)  \n",
        "    rewards = torch.tensor(rewards).to(device).flip(dims= [0])\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "\n",
        "    # Compute actor and critic losses\n",
        "\n",
        "    for G, state, action in zip(returns, states, actions):\n",
        "      G = G.detach() \n",
        "      state = state.unsqueeze(0)\n",
        "      pi = self.actor_net(state)\n",
        "      v = self.critic_net(state)\n",
        "      dist = Categorical(probs = pi)\n",
        "      log_prob = dist.log_prob(action)\n",
        "      advantage = G - v.item() # detach the grad computation to avoid computing gradient\n",
        "      actor_loss = -log_prob * advantage.detach()\n",
        "      critic_loss = F.smooth_l1_loss(v.squeeze(0), torch.tensor([G]).to(device).detach()).unsqueeze(0)\n",
        "      self.critic_optim.zero_grad()\n",
        "      self.actor_optim.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      actor_loss.backward()\n",
        "      self.critic_optim.step()\n",
        "      self.actor_optim.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n",
        "\n",
        "  def learn_backward(self, rewards, states, actions, return_norm = True):\n",
        "    \n",
        "    returns = []\n",
        "    states = torch.cat(states, dim = 0).to(device).flip(dims = [0])\n",
        "    actions = torch.tensor(actions).to(device).flip(dims = [0])\n",
        "    rewards = torch.tensor(rewards).to(device).flip(dims= [0])\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "    returns = returns.flip(dims = [0]) # Flip it again to traverse backward\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "    # Compute actor and critic losses\n",
        "\n",
        "    for G, state, action in zip(returns, states, actions):\n",
        "      G = G.detach() \n",
        "      state = state.unsqueeze(0)\n",
        "      pi = self.actor_net(state)\n",
        "      v = self.critic_net(state)\n",
        "      dist = Categorical(probs = pi)\n",
        "      log_prob = dist.log_prob(action)\n",
        "      advantage = G - v.detach() # detach the grad computation to avoid computing gradient\n",
        "      actor_loss = -log_prob * advantage.detach()\n",
        "      critic_loss = F.smooth_l1_loss(v.squeeze(0), torch.tensor([G]).to(device).detach()).unsqueeze(0)\n",
        "      self.critic_optim.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optim.step()\n",
        "      self.actor_optim.zero_grad()\n",
        "      actor_loss.backward()\n",
        "      self.actor_optim.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKW3_a-IYW5o"
      },
      "source": [
        "# Without Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSwd53TfL_np"
      },
      "source": [
        "import gym\n",
        "import torch\n",
        "import time\n",
        "import pdb\n",
        "\n",
        "def train():\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  env = gym.make('CartPole-v1')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = ActorCritic(input_dims = state_dim,  output_dims = action_dim, ac_lr = 1e-3, cr_lr = 0.01, optimizer= 'Adam').to(device)\n",
        "  num_ep = 1000\n",
        "  print_interval = 100\n",
        "  running_score = 10\n",
        "\n",
        "  for ep in range(num_ep):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      rewards.append(reward)\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "\n",
        "    # train the agent\n",
        "    pdb.set_trace()\n",
        "    agent.learn_backward(rewards, states, actions, return_norm = True)\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So6zlGK4JFJM"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-YfHGWmcpZV"
      },
      "source": [
        "With Wandb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPZePlNPcrJM"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i17q83om2wrJ"
      },
      "source": [
        "import wandb\n",
        "sweep_config = dict()\n",
        "sweep_config['method'] = 'grid'\n",
        "sweep_config['metric'] = {'name': 'running_score', 'goal': 'maximize'}\n",
        "sweep_config['parameters'] = {'learning': {'values': ['learn_mean','learn_forward', 'learn_backward']}, 'actor_learning_rate': {'values' : [0.01, 0.001, 0.0001,0.0003,0.00001]}, 'critic_learning_rate' : {'values': [0.01, 0.001, 0.0001, 0.0003, 0.00001]}\n",
        "                              , 'num_neurons': {'value': 128 }, 'optimizer': {'values' : ['Adam']}}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = 'REINFORCE_Baseline_seperate_net')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQF1Tdkecuv5"
      },
      "source": [
        "import gym \n",
        "import torch\n",
        "import time\n",
        "import wandb\n",
        "\n",
        "def train():\n",
        "  wandb.init(config = {'env':'CartPole-v1','algorithm:': 'REINFORCE_Baseline','architecture': 'seperate','num_laeyrs':'2'}, project = 'REINFORCE_Baseline_seperate_net',group = 'Cart_REINFORCE_Baseline_with_128_seperate')\n",
        "  config = wandb.config\n",
        "  start = time.time()\n",
        "\n",
        "  env = gym.make('CartPole-v1')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = ActorCritic(input_dims = state_dim,  output_dims = action_dim, ac_lr = config.actor_learning_rate, cr_lr = config.critic_learning_rate, optimizer = config.optimizer).to(device)\n",
        "  num_ep = 1000\n",
        "  print_interval = 100\n",
        "  save_interval = 1000\n",
        "  running_score = 10\n",
        "\n",
        "  wandb.watch(agent)\n",
        "  for ep in range(1,num_ep+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      rewards.append(reward)\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "    wandb.log({'episode': ep, 'running_score': running_score})\n",
        "\n",
        "    # train the agent\n",
        "  \n",
        "    if config.learning == 'learn_mean':\n",
        "      agent.learn_mean(rewards,states, actions, return_norm = True)\n",
        "    elif config.learning == 'learn_forward':\n",
        "      agent.learn_forward(rewards, states, actions, return_norm = True)\n",
        "    elif config.learning == 'learn_backward':\n",
        "      agent.learn_backward(rewards, states, actions, return_norm = True)\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))    \n",
        "    \n",
        "\n",
        "    if ep == num_ep:\n",
        "      dummy_input = torch.rand(1,4).to(device)\n",
        "      torch.onnx.export(agent.actor_net,dummy_input,'final_actor.onnx')\n",
        "      torch.onnx.export(agent.critic_net,dummy_input, 'final_critic.onnx')\n",
        "      wandb.save('final_actor.onnx')\n",
        "      wandb.save('final_critic.onnx')\n",
        "      torch.save(agent.actor_net.state_dict(),'final_actor.pt')\n",
        "      wandb.save('final_actor.pt')\n",
        "      torch.save(agent.critic_net.state_dict(),'final_critic.pt')\n",
        "      wandb.save('final_critic.pt')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYhJFKiBd8Hc"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp_j3nWpd1W6"
      },
      "source": [
        "# You can see the result here!\n",
        "[Report Link](https://wandb.ai/ko120/REINFORCE_Baseline/reports/REINFORCE-with-Baseline-forward-and-backward--Vmlldzo4NzM4ODE)"
      ]
    }
  ]
}