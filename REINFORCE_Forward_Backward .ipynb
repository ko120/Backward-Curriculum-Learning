{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya21QxV42_2g"
      },
      "source": [
        "# Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV2TSwV4nMtG"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGAGVm7x3B6U"
      },
      "source": [
        "# Policy Network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki2hesDFUl8x"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, alpha = 0.0001):\n",
        "    super(Policy, self).__init__()\n",
        "    self.fc1 = nn.Linear(in_dim, 128)\n",
        "    self.hidden_act = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(128, out_dim)\n",
        "    self.output_act = nn.Softmax(dim = 1)\n",
        "\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.fc1(state)\n",
        "    x = self.hidden_act(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.output_act(x)\n",
        " \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BPQ3PIZ3E26"
      },
      "source": [
        "# REINFORCE with forward and backward update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3C9dVgOCwYf"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "\n",
        "class REINFORCE(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, alpha = 0.0001, gamma = 0.99):\n",
        "    super(REINFORCE, self).__init__()\n",
        "    self.gamma = gamma\n",
        "    self.policy = Policy(in_dim, out_dim)\n",
        "    self.optimizer = torch.optim.Adam(self.policy.parameters(),lr = alpha)\n",
        "    self.states = []\n",
        "    self.rewards = []\n",
        "    self.actions = []\n",
        "\n",
        "  def save_episode(self, states, actions, rewards):\n",
        "    self.states = torch.cat(states, dim = 0).to(device)\n",
        "    self.actions = torch.tensor(actions).to(device)\n",
        "    self.rewards = torch.tensor(rewards).to(device)\n",
        "  \n",
        "  def get_action(self, state):\n",
        "    with torch.no_grad():\n",
        "      prob = self.policy(state)\n",
        "      distribution = Categorical(probs = prob)\n",
        "      action = distribution.sample()\n",
        "\n",
        "    return action\n",
        "  \n",
        "  def learn_sum_loss(self, norm_return = False): # Episodic update\n",
        "\n",
        "    # Reverse the episode to calculate the return recursively\n",
        "    self.states = self.states.flip(dims = [0])\n",
        "    self.actions = self.actions.flip(dims = [0])\n",
        "    self.rewards = self.rewards.flip(dims = [0])\n",
        "\n",
        "    G = torch.tensor(0).float().to(device)\n",
        "    returns = []\n",
        "    for reward in self.rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0,G)\n",
        "    returns = torch.tensor(returns)\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    \n",
        "    returns = returns.flip(dims = [0]) # Flip it again to traverse backward\n",
        "    if norm_return:\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    losses = []\n",
        "    for state, action, G  in zip(self.states, self.actions, returns):\n",
        "      state = state.unsqueeze(0)\n",
        "      probs = self.policy(state)\n",
        "      distr = Categorical(probs = probs)\n",
        "      log_prob = distr.log_prob(action)\n",
        "      loss = -log_prob * G.detach()\n",
        "      losses.append(loss)\n",
        "    losses = torch.cat(losses).sum().to(device)\n",
        "    self.optimizer.zero_grad()\n",
        "    losses.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # clear the memory\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "\n",
        "  def learn_backward(self, norm_return = False):\n",
        "\n",
        "    # Reverse the episode to calculate the return recursively\n",
        "    self.states = self.states.flip(dims = [0])\n",
        "    self.actions = self.actions.flip(dims = [0])\n",
        "    self.rewards = self.rewards.flip(dims = [0])\n",
        "\n",
        "    G = torch.tensor(0).float().to(device)\n",
        "    returns = []\n",
        "    for reward in self.rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0,G)\n",
        "    returns = torch.tensor(returns)\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    \n",
        "    returns = returns.flip(dims = [0]) # Flip it again to traverse backward\n",
        "    if norm_return:\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    for state, action, G  in zip(self.states, self.actions, returns):\n",
        "      state = state.unsqueeze(0)\n",
        "      probs = self.policy(state)\n",
        "      distr = Categorical(probs = probs)\n",
        "      log_prob = distr.log_prob(action)\n",
        "      loss = -log_prob * G.detach()\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # clear the memory\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "  \n",
        "  def learn_forward(self, norm_return = False):\n",
        "    G = torch.tensor(0).float().to(device)\n",
        "    returns = []\n",
        "    self.rewards = self.rewards.flip([0])\n",
        "    for reward in self.rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns)\n",
        "    eps = np.finfo(np.float32).eps.item()\n",
        "    if norm_return:\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    for state, action, G  in zip(self.states, self.actions, returns):\n",
        "      state = state.unsqueeze(0)\n",
        "      probs = self.policy(state)\n",
        "      distr = Categorical(probs = probs)\n",
        "      log_prob = distr.log_prob(action)\n",
        "      loss = -log_prob * G.detach()\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # clear the memory\n",
        "    self.states = []\n",
        "    self.actions = []\n",
        "    self.rewards = []\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHpFdGqM3NoG"
      },
      "source": [
        "# Wandb is great tool to record machine learning experiment, you can further explore in detail on link below\n",
        "[Wandb](https://wandb.ai/site?gclid=CjwKCAjwlrqHBhByEiwAnLmYUGy29ZdG460eDefcxyto5hte2XmbYPmr59UQdINKtP18J8w2YbbdFxoCS6UQAvD_BwE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqbcInpYWerk"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcniMAAjXzey"
      },
      "source": [
        "import wandb\n",
        "sweep_config = dict()\n",
        "sweep_config['method'] = 'grid'\n",
        "sweep_config['metric'] = {'name': 'running_score', 'goal': 'maximize'}\n",
        "sweep_config['parameters'] = {'direction': {'values': ['backward',]}, 'learning_rate': {'values' : [0.01,0.001,0.0001,0.0003,0.00001]}\n",
        "                              , 'norm_return': {'value': True}}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = 'REINFORCE_CartPole-v1_trajectory_direction')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVN62yQE3v0P"
      },
      "source": [
        "# Environment requirement for LunarLander\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFjeW9jK7Rkn"
      },
      "source": [
        "!pip install box2d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qyP1uew3Pzl"
      },
      "source": [
        "#Without Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bylXze8uZpG"
      },
      "source": [
        "import gym \n",
        "import torch\n",
        "import time\n",
        "\n",
        "def train():\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = REINFORCE(in_dim= state_dim,  out_dim = action_dim).to(device)\n",
        "  num_ep = 3000\n",
        "  print_interval = 100\n",
        "  running_score = -200\n",
        "  solved_scor = 200\n",
        "\n",
        "  \n",
        "  for ep in range(1, num_ep+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "      rewards.append(reward)\n",
        "\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    # save rollout sets\n",
        "    agent.save_episode(states, actions, rewards)\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "\n",
        "    # train the agent\n",
        "    agent.learn_backward()\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))\n",
        "  save_name = 'agent_backward_with_norm_1e-4' + '.pt'\n",
        "    if running_score >= solved_score:\n",
        "      print('Environmnet solved at {} episode with running score of {}' .format(ep, running_score))\n",
        "      break\n",
        "  torch.save(agent.state_dict(),save_name)\n",
        "  wandb.save(save_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLfalWZRvHDy"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eETD730x3Tt9"
      },
      "source": [
        "# Learning rate hyper parementer tuning with Wandb sweep function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SotAfIRiLkNF"
      },
      "source": [
        "import gym \n",
        "import torch\n",
        "import time\n",
        "\n",
        "def train():\n",
        "  wandb.init(config = {'env':'LunarLander-v2','algorithm:': 'REINFORCE_with_norm_backward_sum_loss' },group = 'LunarLander-v2_learning_rate_tune_with_norm_backward_sum_loss')\n",
        "  config = wandb.config\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = REINFORCE(in_dim= state_dim,  out_dim = action_dim, alpha= config.learning_rate).to(device)\n",
        "  num_ep = 3000\n",
        "  print_interval = 100\n",
        "  save_interval = 1000\n",
        "  running_score = 10\n",
        "\n",
        "  wandb.watch(agent)\n",
        "  for ep in range(1,num_ep+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "      rewards.append(reward)\n",
        "\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "    # save rollout sets\n",
        "    agent.save_episode(states, actions, rewards)\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "    wandb.log({'episode': ep, 'running_score': running_score})\n",
        "\n",
        "    # train the agent\n",
        "    if config.direction == 'backward':     \n",
        "      agent.learn_backward(norm_return = config.norm_return)\n",
        "    else:\n",
        "      agent.learn_forward(norm_return = config.norm_return)\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))    \n",
        "    \n",
        "    if ep % save_interval == 0:\n",
        "      save_name = 'agent_' + str(ep) + '.pt'\n",
        "      torch.save(agent.state_dict(),save_name)\n",
        "      wandb.save(save_name)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DtX_r7eROZz"
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHNgoqLG3-rr"
      },
      "source": [
        "# You can check out the result for comparing forward and backward algorithm for REINFORCE with hyper parameter tuning\n",
        "\n",
        "[Link for Report](https://https://wandb.ai/ko120/REINFORCE_CartPole-v1_trajectory_direction/reports/REINFORCE-Updating-direction-variation--Vmlldzo4NDMxMDQ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk62Yc138igS"
      },
      "source": [
        "# Visualize agent before train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D93QynKS8eKB"
      },
      "source": [
        "# For visualization (rendering OpenAI Gym)\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb  \n",
        "!pip install -U colabgymrender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JOBEciw8gbl"
      },
      "source": [
        "# For visualization, must set this up to make virtual displaying screen on Colab, otherwise, it fails\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeU1vMFa8lrQ"
      },
      "source": [
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "directory = '/content/videos'\n",
        "\n",
        "torch.manual_seed(543)\n",
        "\n",
        "env = Recorder(env, directory)\n",
        "\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "agent = REINFORCE(in_dim= state_dim,  out_dim = action_dim).to(device)\n",
        "\n",
        "state = env.reset()\n",
        "terminal = False\n",
        "while not terminal:\n",
        "  state = torch.tensor(state).unsqueeze(0).to(device)\n",
        "  action = agent.get_action(state)\n",
        "  next_state, reward, terminal, info = env.step(action.item())\n",
        "  state = next_state\n",
        "\n",
        "env.play()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp662qRzGU4r"
      },
      "source": [
        "# Visualize best performing forward trained agent (lr= 1e-4) without normalization return\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHzpXm-lGl4Y"
      },
      "source": [
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "directory = '/content/videos'\n",
        "\n",
        "torch.manual_seed(543)\n",
        "\n",
        "env = Recorder(env, directory)\n",
        "\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "agent = REINFORCE(in_dim= state_dim,  out_dim = action_dim).to(device)\n",
        "\n",
        "agent.load_state_dict(torch.load('agent_forward_withoutnorm_1e-4.pt'))\n",
        "agent.eval()\n",
        "\n",
        "\n",
        "state = env.reset()\n",
        "terminal = False\n",
        "while not terminal:\n",
        "  state = torch.tensor(state).unsqueeze(0).to(device)\n",
        "  action = agent.get_action(state)\n",
        "  next_state, reward, terminal, info = env.step(action.item())\n",
        "  state = next_state\n",
        "\n",
        "env.play()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzr2CAoc6hQf"
      },
      "source": [
        "# Visualize best performing backward trained agent (lr= 1e-4) with normalization return\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e5Ob_6gHU7b"
      },
      "source": [
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "directory = '/content/videos'\n",
        "\n",
        "\n",
        "\n",
        "env = Recorder(env, directory)\n",
        "\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "agent = REINFORCE(in_dim= state_dim,  out_dim = action_dim).to(device)\n",
        "\n",
        "agent.load_state_dict(torch.load('agent_backward_with_norm_1e-4.pt'))\n",
        "agent.eval()\n",
        "\n",
        "state = env.reset()\n",
        "terminal = False\n",
        "while not terminal:\n",
        "  state = torch.tensor(state).unsqueeze(0).to(device)\n",
        "  action = agent.get_action(state)\n",
        "  next_state, reward, terminal, info = env.step(action.item())\n",
        "  state = next_state\n",
        "\n",
        "env.play()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}