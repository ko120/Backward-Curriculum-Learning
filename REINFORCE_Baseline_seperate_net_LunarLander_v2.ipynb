{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxsdYOtlchzc"
      },
      "source": [
        "# Enable GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ykkKPog6QG6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obx2tUVVco5a"
      },
      "source": [
        "# Actor Critic Share Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGfFj5m6HUsW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class ActorCritic_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(ActorCritic_Net , self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims,fc1_dims)\n",
        "    self.actor = nn.Linear(fc1_dims, output_dims)\n",
        "    self.critic = nn.Linear(fc1_dims,1)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    pi = F.softmax(self.actor(x), dim = 1)\n",
        "    value = self.critic(x)\n",
        "    return (pi, value)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmhMTk1YtNhH"
      },
      "source": [
        "# Actor Critic Seperate Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrdQzMdgtQSC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "class Actor_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(Actor_Net,self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "    self.out = nn.Linear(fc1_dims, output_dims)\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.out.weight)\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.softmax(self.out(x), dim = 1)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Critic_Net(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128):\n",
        "    super(Critic_Net, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dims, fc1_dims)\n",
        "    self.out = nn.Linear(fc1_dims, 1)\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.xavier_uniform_(self.out.weight)\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = self.out(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WESaGvzgcroP"
      },
      "source": [
        "# REINFORCE with Baseline Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct-q7avTJhWi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "  def __init__(self, input_dims, output_dims, fc1_dims = 128, gamma = 0.99, lr = 1e-4):\n",
        "    super(ActorCritic, self).__init__()\n",
        "    self.ac_net = ActorCritic_Net(input_dims= input_dims, output_dims= output_dims, fc1_dims = fc1_dims)\n",
        "    self.optimizer = optim.Adam(params= self.ac_net.parameters(), lr = lr)\n",
        "    self.gamma = gamma\n",
        "\n",
        "    self.log_probs = []\n",
        "    self.values = []\n",
        "  \n",
        "  def get_action(self, state):\n",
        "    pi, v = self.ac_net(state)\n",
        "    distribution = Categorical(probs = pi)\n",
        "    action = distribution.sample()\n",
        "    self.log_probs.append(distribution.log_prob(action))\n",
        "    self.values.append(v)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "  def get_action(self, state):\n",
        "    pi, v = self.ac_net(state)\n",
        "    distribution = Categorical(probs = pi)\n",
        "    action = distribution.sample()\n",
        "    self.log_probs.append(distribution.log_prob(action))\n",
        "    self.values.append(v)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "  def learn_mean(self, rewards ,states, actions, return_norm = True):\n",
        "    \n",
        "  \n",
        "    returns = []\n",
        "    actor_losses = []\n",
        "    critic_losses = []\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards[::-1]:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "    # Resize the vectors\n",
        "  \n",
        "    self.values = torch.cat(self.values).squeeze() # concatinating plus squeeze since input dim is 2d\n",
        "    self.log_probs = torch.cat(self.log_probs) # only concatinating since input dim is 1d\n",
        "\n",
        "    # Compute actor and critic losses\n",
        "    \n",
        "    for G, log_prob, v in zip(returns, self.log_probs, self.values):\n",
        "      \n",
        "      G = G.detach() \n",
        "      advantage = G - v.item() # detach the grad computation to avoid computing gradient\n",
        "      actor_losses.append(-log_prob * advantage)\n",
        "      critic_losses.append(F.smooth_l1_loss(v, G))\n",
        "    self.optimizer.zero_grad()\n",
        "    loss = (torch.stack(actor_losses).sum()).mean() + (torch.stack(critic_losses).sum()).mean()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n",
        "\n",
        "  def learn_forward(self, rewards, states, actions, return_norm = True):\n",
        "    \n",
        "  \n",
        "    returns = []\n",
        "    states = torch.cat(states, dim = 0).to(device)\n",
        "    actions = torch.tensor(actions).to(device)  \n",
        "    rewards = torch.tensor(rewards).to(device).flip(dims= [0])\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "\n",
        "    # Compute actor and critic losses\n",
        "\n",
        "    for G, state, action in zip(returns, states, actions):\n",
        "      G = G.detach() \n",
        "      state = state.unsqueeze(0)\n",
        "      pi, v = self.ac_net(state)\n",
        "      dist = Categorical(probs = pi)\n",
        "      log_prob = dist.log_prob(action)\n",
        "      advantage = G - v.item() # detach the grad computation to avoid computing gradient\n",
        "      actor_loss = -log_prob * advantage\n",
        "      critic_loss = F.smooth_l1_loss(v, torch.tensor([G]).unsqueeze(0).to(device))\n",
        "      self.optimizer.zero_grad()\n",
        "      loss = actor_loss + critic_loss\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n",
        "\n",
        "  def learn_backward(self, rewards, states, actions, return_norm = True):\n",
        "    \n",
        "    returns = []\n",
        "    states = torch.cat(states, dim = 0).to(device).flip(dims = [0])\n",
        "    actions = torch.tensor(actions).to(device).flip(dims = [0])\n",
        "    rewards = torch.tensor(rewards).to(device).flip(dims= [0])\n",
        "\n",
        "    # Calculate returns\n",
        "    G = 0\n",
        "    for reward in rewards:\n",
        "      G = reward + self.gamma * G\n",
        "      returns.insert(0, G)\n",
        "    returns = torch.tensor(returns).to(device)\n",
        "    returns = returns.flip(dims = [0]) # Flip it again to traverse backward\n",
        "\n",
        "    if return_norm:\n",
        "      eps = np.finfo(np.float32).eps.item()\n",
        "      returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "\n",
        "    # Compute actor and critic losses\n",
        "\n",
        "    for G, state, action in zip(returns, states, actions):\n",
        "      G = G.detach() \n",
        "      state = state.unsqueeze(0)\n",
        "      pi, v = self.ac_net(state)\n",
        "      dist = Categorical(probs = pi)\n",
        "      log_prob = dist.log_prob(action)\n",
        "      advantage = G - v.item() # detach the grad computation to avoid computing gradient\n",
        "      actor_loss = -log_prob * advantage\n",
        "      critic_loss = F.smooth_l1_loss(v, torch.tensor([G]).unsqueeze(0).to(device)).unsqueeze(0)\n",
        "      self.optimizer.zero_grad()\n",
        "      assert actor_loss.size() == critic_loss.size()\n",
        "      loss = actor_loss + critic_loss\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    # clear out the memory\n",
        "    self.values = []\n",
        "    self.log_probs = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6A9tWBfTJl1"
      },
      "source": [
        "#Environment requiremnt for LunarLander -v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi9J_xfxTOj8",
        "outputId": "64ad5e67-9e4e-4bed-8502-604f163977be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: swig in /usr/local/lib/python3.8/dist-packages (4.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.13.0)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.1.0)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.10.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKW3_a-IYW5o"
      },
      "source": [
        "# Without Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSwd53TfL_np"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import time\n",
        "import pdb\n",
        "import wandb\n",
        "\n",
        "def train():\n",
        "  wandb.init(config = {'env':'LunarLander-v2','algorithm:': 'REINFORCE_Baseline_forward','architecture': 'seperate','num_laeyrs':'2'}, project = 'REINFORCE_Baseline_seperate_net_LunarLander-v2',group = 'REINFORCE_Baseline_with_128_seperate_LunarLander-v2')\n",
        "  start = time.time()\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = ActorCritic(input_dims = state_dim,  output_dims = action_dim, fc1_dims= 128, lr = 0.0001).to(device)\n",
        "  num_ep = 3000\n",
        "  print_interval = 100\n",
        "  running_score = 10\n",
        "\n",
        "  for ep in range(num_ep):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      rewards.append(reward)\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "    wandb.log({'episode': ep, 'running_score': running_score})\n",
        "    # train the agent\n",
        "    #pdb.set_trace()\n",
        "    agent.learn_forward(rewards, states, actions, return_norm = True)\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 858,
          "referenced_widgets": [
            "54c7fb6ef21740f3a682d000a55396fe",
            "164f412cd6864798b97bee3c51cc8142",
            "074c06d807004c2694c99f9ba71889be",
            "f0a1f439183248758b261ed258f8a045",
            "b597d6b14c584381a7db76cd0f7913cf",
            "07a8f0c6289b419fb452e80977c468ae",
            "49e4fdc5b1a64777bdfd6fb1cc0ca401",
            "d5fb0b3649e44f05bc5efdb3ce7d328c"
          ]
        },
        "id": "So6zlGK4JFJM",
        "outputId": "85f6803a-2d0f-4f26-a3d7-a9fbd854832d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:2dq793e9) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54c7fb6ef21740f3a682d000a55396fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.160 MB of 0.160 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>running_score</td><td>▁▁▁▁▂▃▂▃▃▅▅▅▇▆▇▇▆▇▆▇▇█▇█▇█▇██▇▇█▇▇▇█▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>2999</td></tr><tr><td>running_score</td><td>188.31003</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">swept-shape-1</strong>: <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/2dq793e9\" target=\"_blank\">https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/2dq793e9</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221204_094115-2dq793e9/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2dq793e9). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221204_104201-1ffq7no3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/1ffq7no3\" target=\"_blank\">stilted-yogurt-2</a></strong> to <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode 0 average reward 4.90757931316575, ended at 0.3\n",
            "episode 100 average reward -154.7003953461933, ended at 40.3\n",
            "episode 200 average reward -130.8306625561131, ended at 80.5\n",
            "episode 300 average reward -120.865426746906, ended at 131.9\n",
            "episode 400 average reward -127.00624485925636, ended at 205.0\n",
            "episode 500 average reward -69.92539738236269, ended at 293.7\n",
            "episode 600 average reward -32.74572212994992, ended at 414.4\n",
            "episode 700 average reward -15.474055639967391, ended at 600.2\n",
            "episode 800 average reward 1.039568641435228, ended at 831.5\n",
            "episode 900 average reward 14.917366459064779, ended at 1095.8\n",
            "episode 1000 average reward 41.37915629090298, ended at 1267.7\n",
            "episode 1100 average reward 18.8907318705555, ended at 1435.8\n",
            "episode 1200 average reward 27.92161920217364, ended at 1603.2\n",
            "episode 1300 average reward 43.00665248672294, ended at 1802.4\n",
            "episode 1400 average reward 13.01791170609769, ended at 1954.5\n",
            "episode 1500 average reward 53.89668157802253, ended at 2201.5\n",
            "episode 1600 average reward 22.94105352825941, ended at 2382.2\n",
            "episode 1700 average reward 35.33537698630873, ended at 2566.6\n",
            "episode 1800 average reward 9.081165716387597, ended at 2661.0\n",
            "episode 1900 average reward 56.09189053679886, ended at 2823.7\n",
            "episode 2000 average reward 30.134496737436347, ended at 3033.0\n",
            "episode 2100 average reward 3.3547534616712325, ended at 3234.3\n",
            "episode 2200 average reward -4.339218996899441, ended at 3421.6\n"
          ]
        }
      ],
      "source": [
        "train() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-YfHGWmcpZV"
      },
      "source": [
        "With Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPZePlNPcrJM"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i17q83om2wrJ",
        "outputId": "10d64616-2e47-4cb6-85e2-0f51f53f68c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: y4vaeztt\n",
            "Sweep URL: https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/sweeps/y4vaeztt\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "sweep_config = dict()\n",
        "sweep_config['method'] = 'grid'\n",
        "sweep_config['metric'] = {'name': 'running_score', 'goal': 'maximize'}\n",
        "sweep_config['parameters'] = {'learning': {'values': ['learn_forward', 'learn_backward']}, 'actor_learning_rate': {'values' : [0.01, 0.001, 0.0001,0.0003,0.00001]}, 'critic_learning_rate' : {'values': [0.01, 0.001, 0.0001, 0.0003, 0.00001]}\n",
        "                              , 'num_neurons': {'value': 128 }, 'optimizer': {'values' : ['Adam']}}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project = 'REINFORCE_Baseline_seperate_net_LunarLander-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQF1Tdkecuv5"
      },
      "outputs": [],
      "source": [
        "import gym \n",
        "import torch\n",
        "import time\n",
        "import wandb\n",
        "\n",
        "def train():\n",
        "  wandb.init(config = {'env':'LunarLander-v2','algorithm:': 'REINFORCE_Baseline','architecture': 'seperate','num_laeyrs':'2'}, project = 'REINFORCE_Baseline_seperate_net_LunarLander-v2',group = 'REINFORCE_Baseline_with_128_seperate_LunarLander-v2')\n",
        "  config = wandb.config\n",
        "  start = time.time()\n",
        "\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env.seed(543)\n",
        "  torch.manual_seed(543)\n",
        "\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.n\n",
        "\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  agent = ActorCritic(input_dims = state_dim,  output_dims = action_dim, ac_lr = config.actor_learning_rate, cr_lr = config.critic_learning_rate, optimizer = config.optimizer).to(device)\n",
        "  num_ep = 3000\n",
        "  print_interval = 100\n",
        "  save_interval = 1000\n",
        "  running_score = 10\n",
        "\n",
        "  wandb.watch(agent)\n",
        "  for ep in range(1,num_ep+1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    done = False\n",
        "    rewards = []\n",
        "    states = []\n",
        "    actions = []\n",
        "    while not done:\n",
        "      state = torch.tensor([state]).float().to(device)\n",
        "      action = agent.get_action(state)\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      \n",
        "      # saving episode\n",
        "      rewards.append(reward)\n",
        "      states.append(state)\n",
        "      actions.append(action.item())\n",
        "\n",
        "      # update score and state\n",
        "      score += reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "  \n",
        "    # calculating score and running score\n",
        "    running_score = 0.05 * score + (1 - 0.05) * running_score\n",
        "    wandb.log({'episode': ep, 'running_score': running_score})\n",
        "\n",
        "    # train the agent\n",
        "  \n",
        "    if config.learning == 'learn_mean':\n",
        "      agent.learn_mean(rewards,states, actions, return_norm = True)\n",
        "    elif config.learning == 'learn_forward':\n",
        "      agent.learn_forward(rewards, states, actions, return_norm = True)\n",
        "    elif config.learning == 'learn_backward':\n",
        "      agent.learn_backward(rewards, states, actions, return_norm = True)\n",
        "\n",
        "    if ep % print_interval == 0:\n",
        "      print('episode {} average reward {}, ended at {:.01f}'.format(ep, running_score, time.time() - start))    \n",
        "    \n",
        "\n",
        "    if ep == num_ep:\n",
        "      dummy_input = torch.rand(1,4).to(device)\n",
        "      torch.onnx.export(agent.actor_net,dummy_input,'final_actor.onnx')\n",
        "      torch.onnx.export(agent.critic_net,dummy_input, 'final_critic.onnx')\n",
        "      wandb.save('final_actor.onnx')\n",
        "      wandb.save('final_critic.onnx')\n",
        "      torch.save(agent.actor_net.state_dict(),'final_actor.pt')\n",
        "      wandb.save('final_actor.pt')\n",
        "      torch.save(agent.critic_net.state_dict(),'final_critic.pt')\n",
        "      wandb.save('final_critic.pt')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a5d78d484f194b64bfe055093813301b"
          ]
        },
        "id": "SYhJFKiBd8Hc",
        "outputId": "3d386685-7988-4e6b-f969-f6979eb62713"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vqaap045 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactor_learning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcritic_learning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning: learn_forward\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mko120\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221202_105907-vqaap045</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/vqaap045\" target=\"_blank\">pretty-sweep-1</a></strong> to <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/sweeps/y4vaeztt\" target=\"_blank\">https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/sweeps/y4vaeztt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode 100 average reward -611.4769293973748, ended at 34.5\n",
            "episode 200 average reward -925.7033412410781, ended at 90.4\n",
            "episode 300 average reward -755.9739274822316, ended at 141.7\n",
            "episode 400 average reward -1223.870050008948, ended at 192.4\n",
            "episode 500 average reward -954.0698186220156, ended at 242.1\n",
            "episode 600 average reward -792.4857167928919, ended at 292.5\n",
            "episode 700 average reward -845.8460096218193, ended at 343.3\n",
            "episode 800 average reward -913.9222422183469, ended at 394.9\n",
            "episode 900 average reward -793.9745881372224, ended at 444.3\n",
            "episode 1000 average reward -817.3260974856521, ended at 493.0\n",
            "episode 1100 average reward -781.9977044996244, ended at 544.6\n",
            "episode 1200 average reward -781.1797906379461, ended at 595.3\n",
            "episode 1300 average reward -838.621673967382, ended at 643.6\n",
            "episode 1400 average reward -936.5563570756482, ended at 698.8\n",
            "episode 1500 average reward -877.8846770013736, ended at 750.3\n",
            "episode 1600 average reward -803.0224726619253, ended at 800.2\n",
            "episode 1700 average reward -825.2979687483952, ended at 849.2\n",
            "episode 1800 average reward -971.3441830887693, ended at 903.6\n",
            "episode 1900 average reward -1116.762914759007, ended at 959.2\n",
            "episode 2000 average reward -852.8085432845619, ended at 1007.9\n",
            "episode 2100 average reward -818.1049051721706, ended at 1054.5\n",
            "episode 2200 average reward -815.8910543311462, ended at 1107.2\n",
            "episode 2300 average reward -961.0862100044782, ended at 1158.1\n",
            "episode 2400 average reward -913.3795444060988, ended at 1208.6\n",
            "episode 2500 average reward -819.3945659333967, ended at 1258.1\n",
            "episode 2600 average reward -808.3797248420902, ended at 1306.6\n",
            "episode 2700 average reward -831.5243690581691, ended at 1356.4\n",
            "episode 2800 average reward -876.3709816172445, ended at 1410.4\n",
            "episode 2900 average reward -842.7013778329414, ended at 1459.8\n",
            "episode 3000 average reward -751.6184636300269, ended at 1510.2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5d78d484f194b64bfe055093813301b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.038 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.019265…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>running_score</td><td>█▅▂▂▃▄▃▃▃▂▃▃▃▃▃▂▃▃▂▂▄▃▄▂▃▁▂▃▃▃▂▃▃▃▃▃▂▄▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode</td><td>3000</td></tr><tr><td>running_score</td><td>-751.61846</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">pretty-sweep-1</strong>: <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/vqaap045\" target=\"_blank\">https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/vqaap045</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20221202_105907-vqaap045/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Run vqaap045 errored: RuntimeError('mat1 and mat2 shapes cannot be multiplied (1x4 and 8x128)')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vqaap045 errored: RuntimeError('mat1 and mat2 shapes cannot be multiplied (1x4 and 8x128)')\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mi3crc08 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactor_learning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tcritic_learning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning: learn_backward\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_neurons: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221202_112440-mi3crc08</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/runs/mi3crc08\" target=\"_blank\">graceful-sweep-2</a></strong> to <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/sweeps/y4vaeztt\" target=\"_blank\">https://wandb.ai/ko120/REINFORCE_Baseline_seperate_net_LunarLander-v2/sweeps/y4vaeztt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "episode 100 average reward -537.5039225203337, ended at 26.6\n",
            "episode 200 average reward -511.5372740474345, ended at 53.0\n",
            "episode 300 average reward -594.300630174898, ended at 80.2\n",
            "episode 400 average reward -608.6068338859022, ended at 107.8\n",
            "episode 500 average reward -538.8779530088104, ended at 135.3\n",
            "episode 600 average reward -568.8609763943414, ended at 162.8\n",
            "episode 700 average reward -560.6892442156437, ended at 189.8\n",
            "episode 800 average reward -560.9704819369373, ended at 216.4\n",
            "episode 900 average reward -586.293070690808, ended at 243.4\n",
            "episode 1000 average reward -510.9530816105201, ended at 270.3\n",
            "episode 1100 average reward -601.5276531726154, ended at 297.8\n"
          ]
        }
      ],
      "source": [
        "wandb.agent(sweep_id, train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0scdynQXEFn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "074c06d807004c2694c99f9ba71889be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49e4fdc5b1a64777bdfd6fb1cc0ca401",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5fb0b3649e44f05bc5efdb3ce7d328c",
            "value": 1
          }
        },
        "07a8f0c6289b419fb452e80977c468ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "164f412cd6864798b97bee3c51cc8142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b597d6b14c584381a7db76cd0f7913cf",
            "placeholder": "​",
            "style": "IPY_MODEL_07a8f0c6289b419fb452e80977c468ae",
            "value": "0.160 MB of 0.160 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "49e4fdc5b1a64777bdfd6fb1cc0ca401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c7fb6ef21740f3a682d000a55396fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_164f412cd6864798b97bee3c51cc8142",
              "IPY_MODEL_074c06d807004c2694c99f9ba71889be"
            ],
            "layout": "IPY_MODEL_f0a1f439183248758b261ed258f8a045"
          }
        },
        "b597d6b14c584381a7db76cd0f7913cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fb0b3649e44f05bc5efdb3ce7d328c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0a1f439183248758b261ed258f8a045": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}